\section{Mathematical preliminaries}
\label{sec:mathematical_preliminaries}

To get started with the \arv and \pyarv packages, we will first need to know at a sufficiently high level what \arv it is doing.
In this \namecref{sec:mathematical_preliminaries} we very briefly present the basic mathematical preliminaries. 

\subsection{A brief mathematical overview}
\label{sec:a_brief_mathematical_overview}

Beginning with a very brief mathematical introduction, we say a random variable \( X \) is described by a probability distribution \( \mathcal{D} \), which details how likely a given outcome is (e.g.\ flipping a coin, rolling a dice, etc.), and denote \( X \) as \textit{following} a distribution \( \mathcal{D} \) by \( X \sim \mathcal{D} \). The probability of a random variable \( X \) taking a value less than or equal to some parameter \( x \)  is described by the \textit{cumulative distribution function} \( C_{\mathcal{D}}(x) \), where \( C(x) \coloneqq \mathbb{P}(X \leq x) \). Supposing we wished to simulate \( n \) random variables \( X_i \sim \mathcal{D} \) for \( i \in \{1,2,\ldots,n\} \), then we could sample the random variable \( X_i \) by computing 
\begin{equation}
\label{eqt:inverse_transform_method}
X_i \coloneq C_{\mathcal{D}}^{-1}(U_i),
\end{equation}
where \( U_i \sim \mathcal{U} \) is a uniform random variable where \( C_{\mathcal{U}}(u) \coloneqq u \) for \( u \in (0, 1) \) \citep[\S\,3.4.1.B]{knuth2019art_semi}. The function \( C^{-1} \) is the \textit{inverse cumulative distribution function}, and \cref{eqt:inverse_transform_method} is known as the \textit{inverse transform method} \citep[\S\,2.2.1]{glasserman2003monte}. Approximate random variables are produced by replacing \( C^{-1} \to \widetilde{C}^{-1} \) in \cref{eqt:inverse_transform_method}, where \( \widetilde{C}^{-1} \) is some approximation to \( C^{-1} \), giving
\begin{equation}
\label{eqt:approximate_inverse_transform_method}
\widetilde{X}_i \coloneq \widetilde{C}_{\mathcal{D}}^{-1}(U_i).
\end{equation} 
We call an \( X \) produced by \cref{eqt:inverse_transform_method} an \textit{exact random variable} following the \textit{exact distribution} \( \mathcal{D} \), and an \( \widetilde{X} \) produced by \cref{eqt:approximate_inverse_transform_method} an \textit{approximate random variable} following the \textit{approximate distribution} \( \widetilde{\mathcal{D}} \). 
The approximations \arv uses are piecewise polynomial approximations on dyadic subintervals, where the polynomials are usually linear or cubic, only a handful of dyadic subintervals are used. A precise definition, construction, analysis, and computational exposition is given in \citep{giles2023approximating}, and a detailed error analysis of their use in multilevel Monte Carlo applications is given in \citep{giles2022approximate}. 

The most important point to take away from this is that to sample approximate random variables from a given distribution, we must first generate samples following a uniform distribution, and then we \textit{transform} these uniform samples into the desired distribution. Notice that in \cref{eqt:inverse_transform_method} and \cref{eqt:approximate_inverse_transform_method}, both transform exact uniform random variables.\footnote{That \cref{eqt:inverse_transform_method} and \cref{eqt:approximate_inverse_transform_method} transform the same uniform random variables is the key for the coupling mechanism in multilevel Monte Carlo applications.} We do not sample from the desired distribution directly. Consequently, there is an extremely subtle and important point of clarification that must be emphasised: \textbf{the \arv and \pyarv packages do not provide random number generators}. What they do provide are libraries which \emph{transform} uniform random variables into approximate random variables approximately following the desired distribution. The random number generators used to generate the uniform random variables must be externally provided \textit{a priori}, and can include pseudo or quasi-random number generators.

Do not be fooled though by the words ``approximation'' and ``linear polynomials'' into thinking that the approximate random variables produced are of a poor or unusable quality, when in fact quite the opposite is true. The quality of the approximations, as measured using a mathematical quantity called the \( L^2 \)-norm, are surprisingly accurate. This measure of quality is quite different to how most libraries measure the quality of their approximations. A ``normal'' library would measure its quality by quoting its error measured using the \( L^\infty \)-norm, corresponding to the ``worst case error''. Instead, by measuring \arv's error using the \( L^2 \)-norm, this corresponds to the ``average case error''. This means that while our library is usually quite accurate, for certain inputs it can have very large errors, but on average these happen so incredibly infrequently for uniformly random inputs that the overall accuracy is still very good. Overall, the linear approximations using only a small numbers of subintervals are very accurate with incredible speed for appropriate applications (e.g.\ Monte Carlo simulations).  If the user wishes to reduce the frequency of the high error cases, they can easily do so by slightly increasing the number of subintervals, and to reduce the typical approximation error, they just increase the polynomial order a small amount. To give a sense of speed, these approximations are typically an order of magnitude (or several) faster than readily available solutions (both proprietary and open source), out competing C++'s Boost, Matlab, Intel's MKL, GNU's GSL, Arm, NAG, C and NumPy's Cephes library, etc.\ \citep[\S\,3.4]{giles2023approximating}. We should emphasise though that convergence is only in the \( L^2 \)-norm. Users who are expecting convergence in any other sense may be sorely mistaken. For example, attempts to plot the convergence in the probability mass functions produce very wild looking results \citep[fig.\,2.1b]{giles2024rounding}!

Readers familiar with methods for efficiently generating Gaussian random variables may ask about other methods besides the inverse transform method, such as the Ziggurat \citep{marsaglia2000ziggurat}, Box-Muller \citep{box1958note}, and Marsaglia \citep{marsaglia1964convenient} methods, which are all heavily used in practice. While reasonable for scalar computational modes, they are ill position for high performance on vectorised SIMD hardware, and a detailed explanation of this is found in \citep[\S\,3.1.1]{giles2023approximating}. In addition to these pitfalls, such methods are restricted to one specific distribution, whereas the inverse transform method is applicable to all probability distributions, and by extension so too are approximate random variables.

Lastly the fact we provide a transformation library, rather than a random number generation library, is an advantage, rather than a short coming. This means the user can input either pseudo random or quasi random variables as inputs. Additionally, this provided a coupling mechanism central for multilevel Monte Carlo simulations. \citet{giles2022approximate} show how this coupling mechanism can be utilised to ensure accuracy is maintained alongside gaining the speed improvements. 

\subsection{Examples of approximate random variables}


The reader may already encountered approximate random variables without realising it, and arguably the simplest possible non-trivial example would be the use of Rademacher random variables \citep{todo}. Rademacher random variables are an approximation of Gaussian random variables and are sampled from \( \{-1, +1\} \) with equal probability. This is an exceptionally crude approximation (where only the mean is correct), but is computationally exceedingly fast. These Rademacher random variables can be seen as a special case of the general piecewise polynomial approximation using dyadic intervals presented in \citep{giles2023approximating}, where there is a single dyadic interval in use, the polynomial is the zeroth order constant approximation, and the constant is chosen to be \( \pm 1 \) for computational convenience rather some other value determined by an \( L^p \) minimisation. Approximating Gaussian random variables with Rademacher random variables for simulating stochastic differential equations is a long standing practice and is known as \textit{the weak Euler-Maruyama scheme} \citep[p.\,\mbox{XXXII}]{glasserman2003monte}.

\subsection{Using low precision approximate random variables with confidence}


For a reader first coming across approximate random variables, especially low precision ones (which might use  e.g.\ half precision), they may have several concerns and reservations. The most typical hesitation is whether they can still be used in applications which don't wish to compromise on accuracy, but wish to benefit from the vast potential for speed improvements. When presented with examples such as the weak Euler-Maruyama scheme and approximations as drastically crude as Rademacher random variables, it's not obvious what level of error may be introduced by switching to approximate random variables, and if this can be mitigated. Thankfully, several of these theoretical and operational issues have undergone a rigorous mathematical analyses, and there are positive and encouraging results to each of these concerns. We very briefly surmise three papers by \citeauthor{giles2022approximate} which directly address these various questions, and refer the reader to them if they want the full mathematical proofs and precise definitions and problem formulations. 

\begin{longdescription}
	\item[Simulation accuracy] \citep{giles2022approximate} demonstrates using approximate random variables in a nested multilevel Monte Carlo framework so their speed benefits are realised whilst full accuracy is maintained. Error bounds are constructed, and limitations surrounding non differentiability analysed. 
	
	\item[Constructing approximations] \citep{giles2023approximating} details the precise construction of approximate random variables, the quality of their approximation, and the implementation details necessary to achieve ultra fast computational performance.
	
	\item[Low precisions] \citep{giles2024rounding} inspects the consequences of moving to low precisions, such as e.g.\ \qty{16}{\bit} half precision, and the accumulation of rounding error. A novel rounding error model is produced, showing that such low precisions can (and should) be utilised in the majority of coarse grained simulations, and that accuracy needn't be compromised. 
\end{longdescription}

Like any technology or mathematical technique, approximate random variables can be misused, but the aforementioned papers can give the reader confidence that they can leverage the \arv and \pyarv packages effectively and safely. Furthermore, these provide precise technical references which they can consult to assure themselves whether they have a setting appropriate for approximate random variables or otherwise. 

While \citep{giles2022approximate,giles2023approximating,giles2024rounding} directly address our specific class of approximate random variables, there is similar (but less specific) literature on the same broader topic. For example, approximations using moment matching schemes are discussed by \citet{muller2015improving}, approximate random variables and dynamic precisions for use on FPGAs is discussed by \citet{haas2025monte}, etc. 
\todo[inline=true]{Present a set of reference material}